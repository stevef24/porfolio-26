---
title: Codex Multi-Agent Orchestrating
description: A practical playbook for running codex like a small engineering team — parallel execution, tight quality gates, and a repeatable loop that stays green.
author: Stav Fernandes
date: 2026-02-19
---

import { codexMultiAgentOrchestratingSteps } from "./codex-multi-agent-orchestrating.steps";

<BlogWithCanvas>

We have all been there. You hand an agent a project, walk away feeling productive, and come back to a confident green summary while real CI is quietly on fire.

The fix is not a better prompt. It is the same fix that works for human teams: distinct jobs, parallel independent work, and a gate before anything merges.

## Think kitchen brigade, not lone chef

One chef trying to prep, cook, plate, and serve every dish will burn something. A brigade — each person owning a station — moves faster and catches mistakes before they leave the pass.

Your codex session works the same way. An **orchestrator** owns the plan, an **explorer** scouts the repo, **implementers** each take one atomic task, a **ci_runner** validates deterministically, and a review layer (**reviewer** + **security_auditor**) catches what the others missed. A **qa_test_author** fills coverage gaps and a **release_manager** handles ship day. The loop: Plan → Implement → CI → Review → Repeat.

<CodexOrchestrationLoop />

Why it works: each role owns one job so prompts stay short and hallucinated side-quests starve. Independent tasks run in parallel instead of blocking a single serial chain. Review and security run on every batch — not stapled to the end.

<AgentCodeWalkthrough steps={codexMultiAgentOrchestratingSteps}>

<CanvasStep index={0}>

## 1) Enable multi-agent in config.toml

Everything starts in `~/.codex/config.toml` (or `.codex/config.toml` for repo-specific overrides). Set `multi_agent = true` under `[features]`, then register each role in `[agents]` with a description and a path to its TOML file.

The practical scaling tip: set `max_threads = 12`. Twelve threads give enough headroom for parallel batches without slamming into rate limits. If you see 429 errors, bring it down — you can always scale up once you trust the loop.

</CanvasStep>

<CanvasStep index={1}>

## 2) The orchestrator — your team lead in TOML

The orchestrator role file is where the loop lives. It declares `model`, `model_reasoning_effort`, `approval_policy`, `sandbox_mode`, and `developer_instructions`.

Instructions tell it to maintain `plan.md`, delegate independent tasks to implementers in parallel, run all three gates after each batch, and convert findings into new tasks. Two rules keep things clean: no scope creep (extras go in a Later section) and small diffs (every update ends with Done / Next / Risks).

The orchestrator runs `gpt-5.3-codex` with `xhigh` reasoning because planning mistakes are the most expensive kind.

</CanvasStep>

<CanvasStep index={2}>

## 3) Workers: explorer and implementer

The explorer maps file paths, finds CI configs, and suggests next probes — but never implements. Keeping exploration separate prevents the classic failure mode where an agent starts "fixing" things it was only supposed to understand.

Implementers do the opposite: one scoped task from `plan.md`, minimal change set, run lint/typecheck/tests, iterate until green, stop. The structured report (what changed, commands run, result) gives the orchestrator the feedback it needs.

Both roles use `gpt-5.3-codex-spark` at medium reasoning effort for fast, cheap execution.

</CanvasStep>

<CanvasStep index={3}>

## 4) Quality gates: CI, review, security

Every batch hits three gates before anything merges. The **ci_runner** mirrors CI locally and outputs pass/fail with root cause analysis. The **reviewer** performs PR-grade review — must-fix issues with file/line refs and a ship-or-block verdict. The **security_auditor** hunts for secrets leakage, injection risks, auth mistakes, and SSRF.

These are hard stops, not suggestions. A batch that cannot pass all three gates is not done. All three share `approval_policy = "never"` and `sandbox_mode = "danger-full-access"`, so the review gate itself is the quality control.

</CanvasStep>

<CanvasStep index={4}>

## 5) Coverage and release roles

The **qa_test_author** fills gaps: adds tests for changed behavior, writes minimal repro steps for bugs, verifies fixes with commands plus expected output. The **release_manager** handles release notes, rollout steps, rollback plan, and a pre-deploy checklist.

You do not need all eight roles on day one. Start with orchestrator, implementer, ci_runner, and reviewer. Add the rest when you feel the gap.

</CanvasStep>

<CanvasStep index={5}>

## 6) Verify and choose your model tier

A quick `ls` and TOML parse confirm codex can read everything. Pro subscribers get `codex-spark` for fast roles (explorer, implementer, ci_runner), while Plus subscribers use `gpt-5.3-codex` across the board. Heavy roles stay on `gpt-5.3-codex`.

The first-run prompt should tell the orchestrator to execute one bounded batch, run all three gates, update `plan.md`, and avoid expanding scope.

</CanvasStep>

</AgentCodeWalkthrough>

## MCP strategy without context bloat

Attach tools to roles, not globally. UI roles get UI MCPs, docs roles get search MCPs, review roles stay lean. If every role sees every tool, you pay in context noise and slower routing.

## Improving over time

After each run, log what broke and what fixed it into an append-only `docs/agent-notes.md`. Promote a lesson into `AGENTS.md` only when it repeats and measurably helps. Keep `AGENTS.md` curated by checking changes against eval outcomes.

## Two starter packs

Pick your risk tolerance. **Pro** runs Spark on fast roles for higher concurrency. **Plus** uses `gpt-5.3-codex` everywhere for reliability.

| Role | Pro | Plus |
| --- | --- | --- |
| orchestrator | gpt-5.3-codex | gpt-5.3-codex |
| explorer | gpt-5.3-codex-spark | gpt-5.3-codex |
| implementer | gpt-5.3-codex-spark | gpt-5.3-codex |
| ci_runner | gpt-5.3-codex-spark | gpt-5.3-codex |
| reviewer | gpt-5.3-codex | gpt-5.3-codex |
| security_auditor | gpt-5.3-codex | gpt-5.3-codex |

## The takeaway

Parallel where it helps. Strict gates where it matters. A single plan to prevent drift. `max_threads = 12` as your starting ceiling. That gives you faster delivery without chaos — and a loop you can trust enough to run unattended.

</BlogWithCanvas>
