---
title: Codex Multi-Agent Orchestrating
description: A practical playbook for running codex like a small engineering team, with parallel execution, tight quality gates, and a repeatable loop that stays green.
author: Stav Fernandes
date: 2026-02-19
---

import { codexMultiAgentOrchestratingSteps } from "./codex-multi-agent-orchestrating.steps";

<BlogWithCanvas>

As AI becomes a bigger part of how we build software, the amount of code we need to review grows with it. The real question is not whether to use these tools. It is how we utilise our time in the most efficient way as we learn to integrate them, and in a way that maximises the value we get back.

When AI coding tools first arrived, everyone thought that better prompts would be the fix for better code. But the actual fix is the same one that works for orchestrating human teams: <Annotate type="underline" color="var(--va-blue)">distinct roles</Annotate>, parallel independent work, and a gate before anything ships.

## Learning to orchestrate

As AI agents progress and become better at writing code, we will write less and less of it ourselves. At OpenAI, [Codex now reviews](https://openai.com/index/introducing-upgrades-to-codex/) the <Annotate type="underline" color="var(--va-blue)">vast majority</Annotate> of pull requests, catching hundreds of issues every day. At Anthropic, engineers report [using Claude Code increasingly](https://www.anthropic.com/research/how-ai-is-transforming-work-at-anthropic) for implementing new features, with that category rising from 14% to 37% in their internal study. I believe that within a year or so, we will spend far less time on smaller details like code style and formatting, and shift our focus toward higher-level design, architecture, and the decisions that actually shape the product.

Think of it like an orchestra. You would never see a conductor trying to play every instrument, read every sheet, and manage the tempo all at once. Instead, there are sections: strings, brass, woodwinds, percussion, each owning their part. The conductor sets the direction, keeps everyone in sync, and decides when to bring each section in.

This is exactly how you should think about your Codex session. We have an **orchestrator** that owns the plan, the conductor setting the direction. We have **explorer** agents that scout the repo like a first chair reading ahead in the score. [Multi-agent workflows](https://developers.openai.com/codex/multi-agent/) are currently experimental in the Codex CLI, but once enabled you can define your own agent roles with role-specific configs and instructions. That is exactly what this post covers.

Below, we will walk through the full setup: the scouts, the implementers, the quality gates, and how they all fit together. The loop is: **plan → implement in parallel → run checks** (tests, typecheck, lint) **→ review/security gate → repeat**.

One thing I learned early on: parallelise read-heavy work first. Multiple agents editing code at the same time can cause merge conflicts, so keep writes scoped and sequential where possible.

<CodexOrchestrationLoop />

### Why this matters

First, each role owns one job, so the prompts can be short and tailored to that agent specifically, instead of reusing the same bloated prompt over and over had we only had a single agent. Second, different tasks require different abilities. When we explore the repository, there is not much deep reasoning involved, which means we can reduce the reasoning effort and in turn increase the speed and efficiency of execution. Below, we will review how to enable multi-agent mode and a quick explanation of how I use it.

To get started, open your Codex config:

```bash
# Open the global codex config (create it if it doesn't exist)
open ~/.codex/config.toml
```

<AgentCodeWalkthrough steps={codexMultiAgentOrchestratingSteps}>

<CanvasStep index={0}>

## 1) Enable multi-agent in config.toml

Everything starts in `~/.codex/config.toml` (or `.codex/config.toml` for repo-specific overrides). Set `multi_agent = true` under `[features]`, then register each role in `[agents]` with a description and a path to its TOML file.

The `agents.max_threads` setting controls how many agent threads can be open concurrently. The default is `6`, but I start at `12` for parallel batches. If you hit rate limits or stability issues, bring it down to `10`, and if you are still seeing errors, try `8`. You can always scale back up once the loop is stable.

The key point here is to experiment and see what works for you. These settings are a starting point, not a prescription.

</CanvasStep>

<CanvasStep index={1}>

## 2) The orchestrator

Let's start with the orchestrator. This is the agent that tells everyone else what to do. Its main job is to maintain the `plan.md` file at the repo root, breaking the work into <Annotate type="underline" color="var(--va-blue)">atomic tasks</Annotate>, delegating them to implementers in parallel, and running all three quality gates after each batch. Atomic tasks work better here because they keep context tight and limit how far an agent can drift off the rails. The smaller and more focused each task is, the easier it is to course-correct when something goes wrong.

Personally, I set this one to the highest reasoning level because the better the plan, the better the outcome. A planning mistake cascades through every batch, so this is where you want the model thinking the hardest.

```bash
# Create the orchestrator agent file
mkdir -p ~/.codex/agents && touch ~/.codex/agents/orchestrator.toml
```

</CanvasStep>

<CanvasStep index={2}>

## 3) The explorer

The explorer maps file paths, finds CI configs, and suggests what to look at next, but it never implements anything. Keeping exploration separate prevents the classic failure mode where an agent starts "fixing" things it was only supposed to understand.

I run this at `low` reasoning. It is just reading files and listing paths. No complex analysis needed, which makes it fast and cheap.

</CanvasStep>

<CanvasStep index={3}>

## 4) The implementer

Implementers do the opposite of the explorer: one scoped task from `plan.md`, minimal change set, run lint/typecheck/tests, iterate until green, then stop. The structured report (what changed, commands run, result) gives the orchestrator the feedback it needs to move on.

I give this `medium` reasoning. The tasks are scoped enough that deep thinking is waste, but it still needs to reason about code changes.

</CanvasStep>

<CanvasStep index={4}>

## 5) The CI runner

The CI runner mirrors CI locally and outputs pass/fail with root cause analysis. It is running deterministic commands and parsing output, so `low` reasoning is all it needs. Think of it as the metronome in our orchestra. It just keeps time.

</CanvasStep>

<CanvasStep index={5}>

## 6) The reviewer

The reviewer performs PR-grade review: must-fix issues with file/line refs and a ship-or-block verdict. This is where you want the model to think deeply, not cut corners. Subtle bugs need deep analysis to surface, so I give it `xhigh` reasoning.

</CanvasStep>

<CanvasStep index={6}>

## 7) The security auditor

The security auditor hunts for secrets leakage, injection risks, auth mistakes, and SSRF. Injection vectors hide in boring code, so this gets `xhigh` reasoning too. Same principle as the reviewer. Thoroughness is the whole point of this role.

</CanvasStep>

<CanvasStep index={7}>

## 8) QA test author

The QA test author fills gaps: adds tests for changed behaviour, writes minimal repro steps for bugs, verifies fixes with commands plus expected output. Edge-case thinking improves coverage, so I give it `high` reasoning.

</CanvasStep>

<CanvasStep index={8}>

## 9) Release manager

The release manager handles release notes, rollout steps, rollback plan, and a pre-deploy checklist. Structured output, not analytical. `medium` reasoning does the job.

You do not need all eight roles on day one. Start with orchestrator, implementer, ci_runner, and reviewer. Add the rest when you feel the gap.

</CanvasStep>

<CanvasStep index={9}>

## 10) Verify and choose your model tier

A quick `ls` and TOML parse confirm codex can read everything. The first-run prompt should tell the orchestrator to execute one bounded batch, run all three gates, update `plan.md`, and avoid expanding scope.

</CanvasStep>

</AgentCodeWalkthrough>

## Benefit one: reasoning effort, spend tokens where they matter

So to recap, here is what I have chosen for my agents and the reason behind each choice. This is not a perfect split, so experiment with it yourself. The principle is simple: roles that **decide** (orchestrator, reviewer, security) get the highest <Annotate type="underline" color="var(--va-blue)">reasoning</Annotate>. Roles that **execute** (explorer, ci_runner) get the lowest. Roles that **create** (implementer, qa, release) sit somewhere in between.

> **Note:** Different memberships have different models available. I have the Pro membership which gives me the ability to use the Spark model (`gpt-5.3-codex-spark`). I have provided an alternative for those with the Plus membership in the chart below.

<ReasoningComplexityChart />

Higher reasoning effort generally increases response time and token usage, but it improves quality on complex work like review and security. By keeping fast roles cheap with lower reasoning, you can afford to run the expensive review roles on every batch without blowing through your budget. Spark (`gpt-5.3-codex-spark`) is ideal for fast, read-heavy work like exploration and CI parsing, while `gpt-5.3-codex` is better suited for deeper analytical work.

## Benefit two: context management with MCP and skills

Every MCP server you enable adds tool context to your messages, so keep your global set lean. That context adds up fast, especially if you have a dozen MCPs attached.

With multi-agent mode, you can use role-specific config files to add heavier MCPs only to the agents that actually need them. The explorer might need file search and docs MCPs. The CI runner just needs terminal access. The reviewer needs nothing beyond the code itself. Each agent only carries the tools relevant to its job.

Instead of every agent paying the context cost of every tool, you scope the overhead to where it matters. Less noise, faster routing, cheaper runs.

The same principle applies to [skills](https://developers.openai.com/codex/skills/). Skills work similarly to MCPs but inject prompt-level instructions rather than tool definitions. They use [progressive disclosure](https://developers.openai.com/codex/concepts/customization/), loading metadata first and full instructions only when chosen. I did not include skills in my config above, but I would highly recommend adding relevant ones to your agent roles. For example, the reviewer could benefit from [Vercel React Best Practices](https://skills.sh/skills/vercel-react-best-practices) or [Next.js Best Practices](https://skills.sh/skills/next-best-practices). The implementer could use [Test-Driven Development](https://skills.sh/skills/test-driven-development). Browse what is available at [skills.sh](https://skills.sh) and pick the ones that match your stack.

## Grab the starter pack

Rather than copying each TOML file individually, here is a setup script that creates the full config in one go. Pick your membership tier and run it.

<CodexStarterPack />

## The takeaway

Parallel where it helps. Strict gates where it matters. A single plan to prevent drift. `max_threads = 12` as your starting ceiling.

Codex is already being built and shipped with an agent harness and parallel orchestration patterns. If you want the deep dive, read OpenAI's posts on [the Codex agent loop](https://openai.com/index/unrolling-the-codex-agent-loop/) and [the Codex harness](https://openai.com/index/unlocking-the-codex-harness/). The way we write software is shifting, and understanding multi-agent orchestration now puts you ahead of the curve.

Take this implementation as an experiment. Tweak the reasoning levels, swap models, add roles, remove roles. But whatever you do, <Annotate type="highlight" color="oklch(0.85 0.15 85 / 0.35)">just experiment</Annotate>.

</BlogWithCanvas>
